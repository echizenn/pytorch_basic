{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d572b8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1058f5f00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "# import numpy as np\n",
    "# import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74904192",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30662b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5.1節のモデル\n",
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = torch.fc2(out)\n",
    "        return out\n",
    "    \n",
    "# kernel_size = 5のモデル\n",
    "class NetNew(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = torch.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e463c",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517ac938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38386"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NetWidth()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28852b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48114"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = NetNew()\n",
    "sum(p.numel() for p in new_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7f88f",
   "metadata": {},
   "source": [
    "conv層の入出力のサイズは変わらないが、カーネルのサイズが大きくなった分、入力画像 \\* カーネルの計算量が増える。その分パラメータも増える。\n",
    "\n",
    "## (b)\n",
    "過学習しやすくなっている。パラメータが増えたから。\n",
    "\n",
    "## (c)\n",
    "urlが少し変わってそう(https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
    "\n",
    "## (d)\n",
    "高さ1幅3のkernelを使うことになる。  \n",
    "paddingもそれに合わせて(0,1)に設定することが推奨される。\n",
    "\n",
    "## (e)\n",
    "このようなカーネルを用いることで、横に長く続く特徴を捉えやすくなる。\n",
    "\n",
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e137100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../chap7/data'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c53c02d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(\"birds_vs_airplanes.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "976a41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                \n",
    "                print(nn.Softmax(dim=1)(outputs))\n",
    "                print(labels)\n",
    "                break\n",
    "                \n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                \n",
    "                total += labels.shape[0]\n",
    "                \n",
    "                correct += int((predicted==labels).sum())\n",
    "#             print(\"Accurary {}: {:.2f}\".format(name, correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ecff1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.0583e-03, 9.9394e-01],\n",
      "        [5.3286e-02, 9.4671e-01],\n",
      "        [9.7912e-01, 2.0877e-02],\n",
      "        [1.3812e-02, 9.8619e-01],\n",
      "        [9.9701e-01, 2.9871e-03],\n",
      "        [3.8812e-01, 6.1188e-01],\n",
      "        [3.3755e-03, 9.9662e-01],\n",
      "        [1.0280e-05, 9.9999e-01],\n",
      "        [9.9670e-01, 3.3042e-03],\n",
      "        [5.1006e-01, 4.8994e-01],\n",
      "        [1.8710e-03, 9.9813e-01],\n",
      "        [1.5123e-02, 9.8488e-01],\n",
      "        [9.7878e-01, 2.1221e-02],\n",
      "        [3.2126e-02, 9.6787e-01],\n",
      "        [5.4583e-02, 9.4542e-01],\n",
      "        [7.2513e-01, 2.7487e-01],\n",
      "        [9.9533e-01, 4.6679e-03],\n",
      "        [2.1662e-01, 7.8338e-01],\n",
      "        [9.6214e-03, 9.9038e-01],\n",
      "        [8.2025e-01, 1.7975e-01],\n",
      "        [3.6460e-01, 6.3540e-01],\n",
      "        [9.5831e-01, 4.1691e-02],\n",
      "        [1.0057e-02, 9.8994e-01],\n",
      "        [5.7914e-04, 9.9942e-01],\n",
      "        [2.0667e-03, 9.9793e-01],\n",
      "        [3.2939e-03, 9.9671e-01],\n",
      "        [3.6856e-01, 6.3144e-01],\n",
      "        [4.2809e-02, 9.5719e-01],\n",
      "        [1.4512e-01, 8.5488e-01],\n",
      "        [9.9298e-01, 7.0168e-03],\n",
      "        [9.9977e-01, 2.3305e-04],\n",
      "        [9.3255e-01, 6.7449e-02],\n",
      "        [9.3106e-01, 6.8942e-02],\n",
      "        [4.1635e-04, 9.9958e-01],\n",
      "        [2.2174e-03, 9.9778e-01],\n",
      "        [2.9860e-01, 7.0140e-01],\n",
      "        [1.6919e-01, 8.3081e-01],\n",
      "        [5.2750e-03, 9.9472e-01],\n",
      "        [8.5683e-01, 1.4317e-01],\n",
      "        [2.3514e-02, 9.7649e-01],\n",
      "        [7.0485e-04, 9.9930e-01],\n",
      "        [6.9717e-03, 9.9303e-01],\n",
      "        [9.4928e-04, 9.9905e-01],\n",
      "        [2.6134e-01, 7.3866e-01],\n",
      "        [8.4196e-01, 1.5804e-01],\n",
      "        [9.4770e-01, 5.2295e-02],\n",
      "        [7.3165e-01, 2.6835e-01],\n",
      "        [7.7538e-01, 2.2462e-01],\n",
      "        [9.6057e-03, 9.9039e-01],\n",
      "        [9.1826e-01, 8.1736e-02],\n",
      "        [3.7600e-03, 9.9624e-01],\n",
      "        [9.3927e-01, 6.0732e-02],\n",
      "        [3.1403e-03, 9.9686e-01],\n",
      "        [9.9699e-01, 3.0112e-03],\n",
      "        [1.0506e-04, 9.9989e-01],\n",
      "        [1.6229e-03, 9.9838e-01],\n",
      "        [1.2553e-02, 9.8745e-01],\n",
      "        [4.5272e-03, 9.9547e-01],\n",
      "        [3.6268e-02, 9.6373e-01],\n",
      "        [9.8384e-01, 1.6164e-02],\n",
      "        [9.9568e-01, 4.3239e-03],\n",
      "        [9.1163e-01, 8.8369e-02],\n",
      "        [9.9214e-01, 7.8594e-03],\n",
      "        [9.9921e-03, 9.9001e-01]])\n",
      "tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6,\n",
      "        2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7, 1, 1, 1, 2,\n",
      "        2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2])\n",
      "tensor([[4.0929e-01, 5.9071e-01],\n",
      "        [9.9957e-01, 4.3227e-04],\n",
      "        [9.9880e-01, 1.1995e-03],\n",
      "        [9.8864e-01, 1.1357e-02],\n",
      "        [2.1499e-05, 9.9998e-01],\n",
      "        [8.3057e-02, 9.1694e-01],\n",
      "        [9.7550e-01, 2.4504e-02],\n",
      "        [2.7637e-01, 7.2363e-01],\n",
      "        [2.7508e-01, 7.2492e-01],\n",
      "        [9.9781e-01, 2.1884e-03],\n",
      "        [9.8631e-01, 1.3694e-02],\n",
      "        [9.6614e-01, 3.3855e-02],\n",
      "        [7.7688e-04, 9.9922e-01],\n",
      "        [3.2824e-01, 6.7176e-01],\n",
      "        [7.8328e-01, 2.1672e-01],\n",
      "        [9.7901e-01, 2.0994e-02],\n",
      "        [9.8535e-04, 9.9901e-01],\n",
      "        [8.3938e-02, 9.1606e-01],\n",
      "        [9.9960e-01, 3.9719e-04],\n",
      "        [5.4209e-03, 9.9458e-01],\n",
      "        [8.5691e-03, 9.9143e-01],\n",
      "        [8.0726e-01, 1.9274e-01],\n",
      "        [6.2776e-01, 3.7224e-01],\n",
      "        [8.0856e-01, 1.9144e-01],\n",
      "        [2.0633e-03, 9.9794e-01],\n",
      "        [9.3496e-04, 9.9907e-01],\n",
      "        [3.4864e-04, 9.9965e-01],\n",
      "        [4.0585e-01, 5.9415e-01],\n",
      "        [3.3529e-01, 6.6471e-01],\n",
      "        [1.1067e-01, 8.8933e-01],\n",
      "        [2.0535e-03, 9.9795e-01],\n",
      "        [6.3484e-03, 9.9365e-01],\n",
      "        [4.4119e-02, 9.5588e-01],\n",
      "        [2.7748e-03, 9.9723e-01],\n",
      "        [8.0090e-01, 1.9910e-01],\n",
      "        [7.5040e-02, 9.2496e-01],\n",
      "        [3.0015e-03, 9.9700e-01],\n",
      "        [9.9960e-01, 4.0023e-04],\n",
      "        [9.9962e-01, 3.8053e-04],\n",
      "        [1.4313e-01, 8.5687e-01],\n",
      "        [9.9841e-01, 1.5851e-03],\n",
      "        [2.1455e-02, 9.7855e-01],\n",
      "        [2.1458e-01, 7.8542e-01],\n",
      "        [2.1056e-04, 9.9979e-01],\n",
      "        [9.9788e-01, 2.1202e-03],\n",
      "        [9.9920e-01, 8.0414e-04],\n",
      "        [5.7710e-03, 9.9423e-01],\n",
      "        [7.3572e-01, 2.6428e-01],\n",
      "        [1.2392e-04, 9.9988e-01],\n",
      "        [1.3173e-03, 9.9868e-01],\n",
      "        [8.9438e-01, 1.0562e-01],\n",
      "        [6.6934e-01, 3.3066e-01],\n",
      "        [6.6143e-02, 9.3386e-01],\n",
      "        [2.6110e-02, 9.7389e-01],\n",
      "        [9.9973e-01, 2.7463e-04],\n",
      "        [8.5369e-01, 1.4631e-01],\n",
      "        [1.1094e-01, 8.8906e-01],\n",
      "        [1.6224e-01, 8.3776e-01],\n",
      "        [8.8628e-02, 9.1137e-01],\n",
      "        [1.8231e-03, 9.9818e-01],\n",
      "        [3.0268e-05, 9.9997e-01],\n",
      "        [2.8457e-04, 9.9972e-01],\n",
      "        [4.0628e-02, 9.5937e-01],\n",
      "        [7.2215e-01, 2.7785e-01]])\n",
      "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
      "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
      "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "validate(loaded_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b219b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f56f6",
   "metadata": {},
   "source": [
    "ラベルが0でも2でもないのに、どちらかの予測確率が95%を超えるものが多数。  \n",
    "\n",
    "## (a)\n",
    "## (b)\n",
    "## (c)\n",
    "省略"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
